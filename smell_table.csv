smell_id,smell_name,smell_Context,smell_Problem,smell_Solution
columns_and_datatype_not_explicitly_set,Columns and DataType Not Explicitly Set,"""In Pandas, all columns are selected by default when a DataFrame is imported from a file or other sources. The data type for each column is defined based on the default dtype conversion.""","If the columns are not selected explicitly, it is not easy for developers to know what to expect in the downstream data schema. If the datatype is not set explicitly, it may silently continue the next step even though the input is unexpected, which may cause errors later. The same applies to other data importing scenarios.",It is recommended to set the columns and DataType explicitly in data processing.
in_place_apis_misused,In-Place APIs Misused,Data structures can be manipulated in mainly two different approaches: 1) by applying the changes to a copy of the data structure and leaving the original object intact or 2) by changing the existing data structure (also known as in-place).,"Some methods can adopt in-place by default, while others return a copy. If the developer assumes an in-place approach, he will not assign the returned value to any variable. Hence, the operation will be executed, but it will not affect the final outcome. For example, when using the Pandas library, the developer may not assign the result of df.dropna() to a variable. He may assume that this API will make changes on the original DataFrame and not set the in-place parameter to be True either. The original DataFrame will not be updated in this way. In the ""TensorFlow Bugs"" replication package, we also found an example where the developer thought np.clip() is an in-place operation and used it without assigning it to a new variable.","We suggest developers check whether the result of the operation is assigned to a variable or the in-place parameter is set in the API. Some developers hold the view that the in-place operation will save memory. However, this is a misconception in the Pandas library because the copy of the data is still created. In PyTorch, the in-place operation does save GPU memory, but it risks overwriting the values needed to compute the gradient."
pytorch_call_method_misused,Pytorch Call Method Misused,Both self.net() and self.net.forward() can be used to forward the input into the network in PyTorch.,"In PyTorch, self.net() and self.net.forward() are not identical. The self.net() also deals with all the register hooks, which would not be considered when calling the plain .forward().",It is recommended to use self.net() rather than self.net.forward().
tensor_array_not_used,TensorArray Not Used,Developers may need to change the value of the array in the loops in TensorFlow.,"If the developer initializes an array using tf.constant() and tries to assign a new value to it in the loop to keep it growing, the code will run into an error. The developer can fix this error by the low-level tf.while\_loop() API. However, it is inefficient coding in this way. A lot of intermediate tensors are built in this process.",Using tf.TensorArray() for growing array in the loop is a better solution for this kind of problem in TensorFlow 2.
Chain_Indexing,Chain Indexing,"In Pandas, df[""one""][""two""] and df.loc[:,(""one"",""two"")] give the same result. The df[""one""][""two""] is called chain indexing.","Using chain indexing may cause performance issues as well as prone-to-bug code. For example, when using df[""one""][""two""], Pandas sees this operation as two events: call df[""one""] first and call [""two""] based on the result the previous operation gets. On the contrary, df.loc[:,(""one"",""two"")] only perform a single call. In this way, the second approach can be significantly faster than the first one. Furthermore, assigning to the product of chain indexing has inherently unpredictable results. Since Pandas makes no guarantees on whether df[""one""] will return a view or a copy, the assignment may fail.",Developers using Pandas should avoid using chain indexing.
dataframe_conversion_api_misused,Dataframe Conversion API Misused,"In Pandas, df.to_numpy() and df.values() both can turn a DataFrame to a NumPy array.","As noted in a Stack Overflow post, df.values() has an inconsistency problem. With .values() it is unclear whether the returned value would be the actual array, some transformation of it, or one of the Pandas custom arrays. However, the .values() API has not been not deprecated yet. Although the library developers note it as a warning in the documentation, it does not log a warning or error when compiling the code if we use .value().","When converting DataFrame to NumPy array, it is better to use df.to_numpy() than df.values()."
Matrix_multiplication_api_misused,Matrix Multiplication API Misused,"When the multiply operation is performed on two-dimensional matrixes, np.matmul() and np.dot() give the same result, which is a matrix.","In mathematics, the result of the dot product is expected to be a scalar rather than a vector. The np.dot() returns a new matrix for two-dimensional matrixes multiplication, which does not match with its mathematics semantics. Developers sometimes use np.dot() in scenarios where it is not supposed to, e.g., two-dimensional multiplication.",When the multiply operation is performed on two-dimensional matrixes
gradients_not_cleared_before_backward_propagation,Gradients Not Cleared before Backward Propagation,"In PyTorch, optimizer.zero_grad() clears the old gradients from last step, loss_fn.backward() does the back propagation, and optimizer.step() performs weight update using the gradients.","If optimizer.zero_grad() is not used before loss_fn.backward(), the gradients will be accumulated from all loss_fn.backward() calls and it will lead to the gradient explosion, which fails the training.","Developers should use optimizer.zero_grad(), loss_fn.backward(), optimizer.step() together in order and should not forget to use optimizer.zero_grad() before loss_fn.backward()."
merge_api_parameter_not_explicitly_set,Merge API Parameter Not Explicitly Set,The df.merge() API merges two DataFrames in Pandas.,"""Although using the default parameter can produce the same result, explicitly specify on and how produce better readability. The parameter on states which columns to join on, and the parameter how describes the join method (e.g., outer, inner). Also, the validate parameter will check whether the merge is of a specified type. If the developer assumes the merge keys are unique in both left and right datasets, but that is not the case, and he does not specify this parameter, the result might silently go wrong. The merge operation is usually computationally and memory expensive. It is preferable to do the merging process in one stroke for performance consideration.""",Developer should explicitly specify the parameters for merge operation.
deterministic_algorithm_option_not_used,Deterministic Algorithm Option Not Used,Using deterministic algorithms can improve reproducibility.,"The non-deterministic algorithm cannot produce repeatable results, which is inconvenient for debugging."," Some libraries provide APIs for developers to use the deterministic algorithm. In PyTorch, it is suggested to set torch.use_deterministic_algorithms(True) when debugging. However, the application will perform slower if this option is set, so it is suggested not to use it in the deployment stage."
empty_column_misinitialization,Empty Column Misinitialization,Developers may need a new empty column in DataFrame.,"If they use zeros or empty strings to initialize a new empty column in Pandas, the ability to use methods such as .isnull() or .notnull() is retained. This might also happens to initializations in other data structure or libraries.",Use NaN value (e.g. np.nan) if a new empty column in a DataFrame is needed. Do not use “filler values” such as zeros or empty strings.
nan_equivalence_comparison_misused,NaN Equivalence Comparison Misused,NaN equivalence comparison behaves differently from None equivalence comparison.,"While None == None evaluates to True, np.nan == np.nan evaluates to False in NumPy. As Pandas treats None like np.nan for simplicity and performance reasons, a comparison of DataFrame elements with np.nan always returns False. If the developer is not aware of this, it may lead to unintentional bugs in the code.",Developers need to be careful when using the NaN comparison.
memory_not_freed,Memory not Freed,"Machine learning training is memory-consuming, and the machine’s memory is always limited by budget.","If the machine runs out of memory while training the model, the training will fail.","Some APIs are provided to alleviate the run-out-of-memory issue in deep learning libraries. TensorFlow’s documentation notes that if the model is created in a loop, it is suggested to use clear_session() in the loop. Meanwhile, the GitHub repository pytorch-styleguide recommends using .detach() to free the tensor from the graph whenever possible. The .detach() API can prevent unnecessary operations from being recorded and therefore can save memory. Developers should check whether they use this kind of APIs to free the memory whenever possible in their code."
unnecessary_iteration,Unnecessary Iteration,"Loops are typically time-consuming and verbose, while developers can usually use some vectorized solutions to replace the loops.","As stated in the Pandas documentation: “Iterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed and can be avoided”. In EffectiveTensorflow github repository, it is also stated that the slicing operation with loops in TensorFlow is slow, and there is a substitute for better performance.","Machine learning applications are typically data-intensive, requiring operations on data sets rather than an individual value. Therefore, it is better to adopt a vectorized solution instead of iterating over data. In this way, the program runs faster and code complexity is reduced, resulting in more efficient and less error-prone code. Pandas’ built-in methods (e.g., join, groupby) are vectorized. It is therefore recommended to use Pandas built-in methods as an alternative to loops. In TensorFlow, using the tf.reduce_sum() API to perform reduction operation is much faster than combining slicing operation and loops."
broadcasting_feature_not_used,Broadcasting Feature Not Used,Use the broadcasting feature in TensorFlow 2 to be more memory efficient.,"Without broadcasting, tiling a tensor first to match another tensor consumes more memory due to the creation and storage of a middle tiling operation result.","With broadcasting, it is more memory efficient. However, there is a trade-off in debugging since the tiling process is not explicitly stated. Therefore, it is suggested to be as explicit as possible in operation to alleviate the debugging problem. For example, specify the dimension in reduction operations and when using tf.squeeze()."
